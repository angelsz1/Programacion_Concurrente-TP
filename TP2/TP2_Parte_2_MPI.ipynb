{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP2 Parte 2 - MPI"
      ],
      "metadata": {
        "id": "UbF2cVssjsq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Ejercicio - Hola Mundo con MPI"
      ],
      "metadata": {
        "id": "d9JsV0Xlj4DB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Preguntas del ejercicio\n",
        "Ejecute este ejemplo para  4  o más instancias y responda:\n",
        "\n",
        "a) **¿Qué función realiza la instancia maestra? ¿Qué función realizan las instancias esclavas?**\n",
        "\n",
        "🔷La función que realiza la instancia maestra es initWork con los siguientes parámetros:\n",
        "* comm: instancia del comunicador MPI, en este caso MPI.COMM_WORLD, que incluye a todos los procesos involucrados en la ejecución paralela.\n",
        "* workTimes: array de enteros, que representarian los trabajos que se les va a asignar a los esclavos.\n",
        "* numProcesses: número de procesos totales ejecutados, contando igualmente al proceso maestro.\n",
        "\n",
        "La función se compone de 4 partes. Para explicarlo mejor, me baso en esta situación: tengo 4 esclavos y 12 trabajos. Los trabajos son t0, t1, ... t11. Los esclavos son p1, p2, p3, p4 (ya que p0 sería el maestro).\n",
        "\n",
        "Primero, \"asigna\" a cada esclavo un trabajo (en este contexto, sus horas de descanso del empleado), es decir, envía una posición del array a su respectivo proceso.\n",
        "p1 -> t0 (\"trabajará en\"),\n",
        "p2 -> t1,\n",
        "p3 -> t2,\n",
        "p4 -> t3\n",
        "\n",
        "Segundo, los trabajos restantes se van asignando a medida que terminan los esclavos sus trabajos previos. El maestro se queda esperando a recibir los resultado del trabajo terminado de los esclavos para asignarles el siguiente.\n",
        "Continuando con el ejemplo, t4 a t11 no habían sigo asignados. Si p3 (no importa que esclavo, MPI.ANY_SOURCE) termina, se le asigna t4. Así, hasta que todos los trabajos hayan sido asignados.\n",
        "\n",
        "Tercero, una vez que todos los trabajos fueron asignados, el maestro también se encarga de recibir estos últimos trabajos a pesar de que ya no tiene que asignar más. En el ejemplo, se quedaría esperando los trabajos terminados t8, t9, t10 y t11.\n",
        "\n",
        "Cuarto y último, notifica a los esclavos que se completaron todos los trabajos por medio de send.\n",
        "\n",
        "🔷La función que realiza las instancias esclavas es doWork con el siguientes parámetro:\n",
        "* comm: instancia del comunicador MPI.\n",
        "El proceso esclavo se queda esperando lo que el proceso maestro le envió. Se mantiene descansando (sleep) según ese valor recibido y envia su resultado (el mismo valor). Si el tag corresponde al fin de trabajo, finaliza. Caso contrario, vuelve a quedar a la escucha.\n",
        "\n",
        "b) **¿Cuántas de esas instancias ejecuta la función main(), initWork() y doWork()?**\n",
        "\n",
        "Siendo N el número total de procesos que participan.\n",
        "La función main() es ejecutado N veces, osea por todos los procesos.\n",
        "La función initWork() una vez, solamente por el proceso maestro, cuyo id = 0.\n",
        "La función doWork() es ejecutado por todos los procesos esclavos. Es decir será ejecutada N-1 veces.\n",
        "\n",
        "c) **¿Cómo se diferencian los mensajes de trabajo o de finalización?**\n",
        "\n",
        "Se diferencia por medio del tag.\n",
        "\n",
        "Los esclavos envian su resultado al maestro sin indicar un tag particular, por lo que se manda un default, el cual el maestro recibe.\n",
        "workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "\n",
        "WORK_FLAG = 1. El maestro envia el trabajo con tag WORK_FLAG.\n",
        "\n",
        "END_WORK_FLAG = 2. El maestro envia el trabajo \"0\" con tag END_WORK a todos los procesos esclavos para avisarles que ya no quedan trabajos que atender.\n",
        "\n",
        "Los esclavos lo reciben cualquiera sea el tag (MPI.ANY_TAG) y trabajan (bajo este contexto, descansan según ese valor, pudiendo ser 0, min_tiempo_sleep a max_tiempo_sleep). A continuación, si el tag resultaba ser END_WORK, finalizan.\n",
        "\n",
        "d) **¿Cómo implementaría la función BLAS axpy() con este patrón?¿Sería eficiente? Tips: Pide solo el planteo, no la implementación.**\n",
        "\n",
        "La operación axpy() es una operación lineal a partir de un escalar y 2 vectores.\n",
        "\n",
        "*Y <- aX + Y*\n",
        "\n",
        "MPI_Scatter(&sendbuf, sendcnt, sendtype, &recvbuf, ...)\n",
        "\n",
        "MPI_Gather(&sendbuf, sendcnt, sendtype, &recvbuf, ...)\n",
        "\n",
        "Podemos tener 2 procesos X e Y. A partir de MPI_Scatter distribuimos a cada proceso un vector. Paralelamente, se haría el cálculo axpy localmente posición a posición.\n",
        "A través de MPI_Gather, el proceso Y recibe los resultados \"parciales\" que sería nuestro vector Y resultado.\n",
        "\n",
        "e) **¿Qué sucede cuando solo ejecuta con una sola instancia?**\n",
        "\n",
        "Al definir NRO = 1, indicamos que la cantidad de procesos que se ejecutarán en paralelo sea uno. De esta forma, comm.Get_size() devuelve 1 efectivamente. El proceso maestro es parte, por lo que no hay \"empleados disponibles\". En el código, tras querer repartir el trabajo a 0 empleados, el numero de tareas es 0.\n",
        "\n",
        "numTasks = (numProcesses-1)*4\n",
        "\n",
        "numTasks = (1-1)*4 = 0\n",
        "\n",
        "f) Punto opcional: El código que ejecutan las instancias esclavas, tienen un error en su lógica. ¿Cómo se podría solucionar?"
      ],
      "metadata": {
        "id": "dUT7agILkKrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Armado del ambiente"
      ],
      "metadata": {
        "id": "yNdfhvbakUDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz9tN-lljqoj",
        "outputId": "c1ec9131-eeb2-4005-aacd-a7be6d556b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746527 sha256=890d6c838ec8a722078a70fad58703a5dcc61b4ecdb05bae3e538b8855aa7b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.5\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. Código del programa en Lenguaje Python"
      ],
      "metadata": {
        "id": "c_l-CpO7jrxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Ejercicio2.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --------------------------------------------\n",
        "# Formulario\n",
        "Max_tiempo_sleep =   1#@param {type: \"slider\", min: 1, max: 10}\n",
        "Min_tiempo_sleep =   0#@param {type: \"slider\", min: 0, max: 10}\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------------------------------------------\n",
        "# Constantes de comunicacion\n",
        "WORK_FLAG = 1\n",
        "END_WORK_FLAG = 2\n",
        "# --------------------------------------------\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD # Instanciamos el tipo de comunicador a utilizar.\n",
        "    id = comm.Get_rank()  # Obtenemos el id como atributo del proceso que se ejecuta.\n",
        "\n",
        "    # Utilizamos el 0 para definir al procesos Maestro, cualquier otro id sera un esclavo.\n",
        "    if (id == 0) :\n",
        "        init() # Llamamos funcion init para eventos que requeriremos inicialmente solo 1 vez.\n",
        "        numProcesses = comm.Get_size()  # Obtenemos el numero de procesos totales ejecutados.\n",
        "        numTasks = (numProcesses-1)*4 # Se setea el numero de tareas.\n",
        "        workTimes = generateTasks(numTasks) # Se generan las tareas, en este caso seran\n",
        "        print(\"El jefe crea {} horas de descanso de sus empleados:\".format(workTimes.size), flush=True)\n",
        "        print(workTimes, flush=True)\n",
        "        initWork(comm, workTimes, numProcesses)\n",
        "    else:\n",
        "        doWork(comm)\n",
        "\n",
        "def generateTasks(numTasks):\n",
        "    #TODO: Cambiar la semilla del random para que se generen efectivamente diferentes numeros.\n",
        "    np.random.seed(1000)\n",
        "    return np.random.randint(low=Min_tiempo_sleep, high=Max_tiempo_sleep, size=numTasks)\n",
        "\n",
        "def init():\n",
        "  print()\n",
        "  print(\"Version MPI4py utilizada: {}\".format(MPI.Get_version()), flush=True)\n",
        "  print()\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print( \"Sistema de trabajo Suizo:\", flush=True)\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print()\n",
        "\n",
        "def initWork(comm, workTimes, numProcesses):\n",
        "    totalWork = workTimes.size\n",
        "    workcount = 0\n",
        "    recvcount = 0\n",
        "\n",
        "    print(\"Jefe enviando las tareas iniciales:\", flush=True)\n",
        "    for id in range(1, numProcesses):\n",
        "        if workcount < totalWork:\n",
        "            work=workTimes[workcount]\n",
        "            comm.send(work, dest=id, tag=WORK_FLAG) # Envia mensaje de iniciar trabajo con el dato correspondiente del array.\n",
        "            workcount += 1\n",
        "            print(\"Jefe envia trabajo y {} hs de descanso al empleado {}.\".format(work, id), flush=True)\n",
        "    print( \"------------------------------------\", flush=True)\n",
        "\n",
        "    # Mientras haya trabajo, se recibe el resultado de los empleados y se sigue enviando MAS trabajo.\n",
        "    while (workcount < totalWork) :\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat) # Recivimos resultados de los empleados.\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source() # Obtenemos el identificador del empleado.\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "        #send next work\n",
        "        work=workTimes[workcount]\n",
        "        comm.send(work, dest=workerId, tag=WORK_FLAG)\n",
        "        workcount += 1\n",
        "        print(\"Jefe envia nuevo trabajo y {} hs de descanso al empleado {}.\".format(work, workerId), flush=True)\n",
        "\n",
        "    while (recvcount < totalWork):\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source()\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "\n",
        "    for id in range(1, numProcesses):\n",
        "        comm.send(0, dest=id, tag=END_WORK_FLAG)\n",
        "\n",
        "\n",
        "def doWork(comm):\n",
        "    while(True):\n",
        "        stat = MPI.Status() # Obtiene el estado actual del empleado.\n",
        "        waitTime = comm.recv(source=0, tag=MPI.ANY_TAG, status=stat) # Obtiene lo enviado por el Jefe.\n",
        "        print(\"Soy el empleado con id {}, toca descanzo por {} hs.\".format(comm.Get_rank(), waitTime), flush=True)\n",
        "\n",
        "        if (stat.Get_tag() == END_WORK_FLAG):\n",
        "            print(\"Marca tarjeta el empleado {}.\".format(comm.Get_rank()), flush=True)\n",
        "            return\n",
        "        time.sleep(waitTime)\n",
        "        comm.send(waitTime, dest=0)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG-rAG5_kiA_",
        "outputId": "47df3b01-d38e-4607-94fc-4cfab9e7b810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Ejercicio2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Ejecución del programa"
      ],
      "metadata": {
        "id": "oumTsozbkqdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parámetros de ejecución"
      ],
      "metadata": {
        "id": "ydfua9dxkvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "#@title Parámetros de ejecución { vertical-output: true }\n",
        "NRO =   6#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python Ejercicio2.py"
      ],
      "metadata": {
        "id": "K17N9DXPkxMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Ejercicio Contar palabras"
      ],
      "metadata": {
        "id": "7H5zkFo7vPDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desarrollar un programa que permita obtener el valor máximo de un conjunto dado de forma distribuida.\n",
        "\n",
        "Las condiciones a tener en cuenta son:\n",
        "\n",
        "Debe trabajar con al menos, 4 procesos.\n",
        "El resultado final debe ser informado en cada proceso.\n",
        "Implementar comunicación por Buffer"
      ],
      "metadata": {
        "id": "-FkLl9zxvRsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_tp4.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "count = 12\n",
        "sendbuf = None\n",
        "recvbuf_ = None # resultado reduce\n",
        "\n",
        "if rank == 0:\n",
        "  sendbuf = np.empty([size, count], dtype='i') # creo matriz\n",
        "  sendbuf[:,:] = np.random.randint(0, 100, size=(size, count)) # asigno valores\n",
        "\n",
        "  print(\"Matriz generada\\n\", sendbuf)\n",
        "  print(\"Maximo check = \", sendbuf.max()) # este debería ser el resultado final.\n",
        "\n",
        "## ---------------------- SCATTER ----------------------------------------------\n",
        "\n",
        "recvbuf = np.empty(count, dtype='i') # array vacio, fila\n",
        "\n",
        "comm.Scatter(sendbuf, recvbuf, root=0) # scatter(matriz, vector, root)\n",
        "print(f\"Scatter | Proceso: {rank} - Data: {recvbuf}\")\n",
        "\n",
        "# todos los procesos recibieron una parte de la matriz \"sendbuf\",\n",
        "# y lo alojan en un array previamente vacio y secuencial \"recvbuf\"\n",
        "\n",
        "## ------------------------ REDUCE ---------------------------------------------\n",
        "\n",
        "if rank == 0:\n",
        "  recvbuf_ = np.empty([1, count], dtype='i') #matriz de fila 1\n",
        "\n",
        "comm.Reduce(recvbuf, recvbuf_, op=MPI.MAX, root=0) # reduce(vector, matriz, op, root)\n",
        "\n",
        "if rank == 0:\n",
        "  print(\"Reduce | Soy proceso root. Maximos de cada columna: \", recvbuf_) #resultado matriz\n",
        "\n",
        "# el proceso root recibe el array \"recvbuf\" de cada proceso y\n",
        "# junto con la operación para obtener los máximos de cada COLUMNA de la matriz\n",
        "# se aloja el resultado en la matriz de una fila \"recvbuf_\".\n",
        "\n",
        "## ----------------------- BROADCAST -------------------------------------------\n",
        "\n",
        "if rank == 0:\n",
        "  max_final = recvbuf_.max() # variable con el maximo de maximos\n",
        "  data = np.array([max_final], dtype='i')\n",
        "else:\n",
        "  data = np.empty(1, dtype='i') # array de una sola posición\n",
        "\n",
        "comm.Bcast(data, root=0)\n",
        "\n",
        "print(f\"Broadcast | Proceso: {rank} - Resultado final: {data[0]}\")\n",
        "\n",
        "# todos los procesos reciben el resultado final y lo informan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaB1xiB6vg0i",
        "outputId": "2c021353-2e72-4685-c227-e92a3552f0f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_tp4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NRO = 4\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python mpi_tp4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxEzCTcwWHb",
        "outputId": "3e356396-e5c8-48f1-9908-46637f22169a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz generada\n",
            " [[69 88 62 54 66 43 31 52 17 20 22 69]\n",
            " [20 11  6 84 14 43 83 54 29 34 50 31]\n",
            " [94 23 19 29 27 17  0  3 89 75 13 83]\n",
            " [ 1 53 75 49 82 89 49 53 26 79 68 73]]\n",
            "Maximo check =  94\n",
            "Scatter | Proceso: 0 - Data: [69 88 62 54 66 43 31 52 17 20 22 69]\n",
            "Scatter | Proceso: 1 - Data: [20 11  6 84 14 43 83 54 29 34 50 31]\n",
            "Scatter | Proceso: 2 - Data: [94 23 19 29 27 17  0  3 89 75 13 83]\n",
            "Scatter | Proceso: 3 - Data: [ 1 53 75 49 82 89 49 53 26 79 68 73]\n",
            "Reduce | Soy proceso root. Maximos de cada columna:  [[94 88 75 84 82 89 83 54 89 79 68 83]]\n",
            "Broadcast | Proceso: 0 - Resultado final: 94\n",
            "Broadcast | Proceso: 1 - Resultado final: 94\n",
            "Broadcast | Proceso: 3 - Resultado final: 94\n",
            "Broadcast | Proceso: 2 - Resultado final: 94\n"
          ]
        }
      ]
    }
  ]
}