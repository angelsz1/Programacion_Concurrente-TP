{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP2 Parte 2 - MPI"
      ],
      "metadata": {
        "id": "UbF2cVssjsq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Ejercicio - Hola Mundo con MPI"
      ],
      "metadata": {
        "id": "d9JsV0Xlj4DB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Preguntas del ejercicio\n",
        "Ejecute este ejemplo para  4  o m谩s instancias y responda:\n",
        "\n",
        "a) **驴Qu茅 funci贸n realiza la instancia maestra? 驴Qu茅 funci贸n realizan las instancias esclavas?**\n",
        "\n",
        "La funci贸n que realiza la instancia maestra es initWork con los siguientes par谩metros:\n",
        "* comm: instancia del comunicador MPI, en este caso MPI.COMM_WORLD, que incluye a todos los procesos involucrados en la ejecuci贸n paralela.\n",
        "* workTimes: array de enteros, que representarian los trabajos que se les va a asignar a los esclavos.\n",
        "* numProcesses: n煤mero de procesos totales ejecutados, contando igualmente al proceso maestro.\n",
        "\n",
        "La funci贸n se compone de 4 partes. Para explicarlo mejor, me baso en esta situaci贸n: tengo 4 esclavos y 12 trabajos. Los trabajos son t0, t1, ... t11. Los esclavos son p1, p2, p3, p4 (ya que p0 ser铆a el maestro).\n",
        "\n",
        "Primero, \"asigna\" a cada esclavo un trabajo (en este contexto, sus horas de descanso del empleado), es decir, env铆a una posici贸n del array a su respectivo proceso.\n",
        "p1 -> t0 (\"trabajar谩 en\"),\n",
        "p2 -> t1,\n",
        "p3 -> t2,\n",
        "p4 -> t3\n",
        "\n",
        "Segundo, los trabajos restantes se van asignando a medida que terminan los esclavos sus trabajos previos. El maestro se queda esperando a recibir los resultado del trabajo terminado de los esclavos para asignarles el siguiente.\n",
        "Continuando con el ejemplo, t4 a t11 no hab铆an sigo asignados. Si p3 (no importa que esclavo, MPI.ANY_SOURCE) termina, se le asigna t4. As铆, hasta que todos los trabajos hayan sido asignados.\n",
        "\n",
        "Tercero, una vez que todos los trabajos fueron asignados, el maestro tambi茅n se encarga de recibir estos 煤ltimos trabajos a pesar de que ya no tiene que asignar m谩s. En el ejemplo, se quedar铆a esperando los trabajos terminados t8, t9, t10 y t11.\n",
        "\n",
        "Cuarto y 煤ltimo, notifica a los esclavos que se completaron todos los trabajos por medio de send.\n",
        "\n",
        "La funci贸n que realiza las instancias esclavas es doWork con el siguientes par谩metro:\n",
        "* comm: instancia del comunicador MPI.\n",
        "El proceso esclavo se queda esperando lo que el proceso maestro le envi贸. Se mantiene descansando (sleep) seg煤n ese valor recibido y envia su resultado (el mismo valor). Si el tag corresponde al fin de trabajo, finaliza. Caso contrario, vuelve a quedar a la escucha.\n",
        "\n",
        "b) **驴Cu谩ntas de esas instancias ejecuta la funci贸n main(), initWork() y doWork()?**\n",
        "\n",
        "Siendo N el n煤mero total de procesos que participan.\n",
        "La funci贸n main() es ejecutado N veces, osea por todos los procesos.\n",
        "La funci贸n initWork() una vez, solamente por el proceso maestro, cuyo id = 0.\n",
        "La funci贸n doWork() es ejecutado por todos los procesos esclavos. Es decir ser谩 ejecutada N-1 veces.\n",
        "\n",
        "c) **驴C贸mo se diferencian los mensajes de trabajo o de finalizaci贸n?**\n",
        "\n",
        "Se diferencia por medio del tag.\n",
        "\n",
        "Los esclavos envian su resultado al maestro sin indicar un tag particular, por lo que se manda un default, el cual el maestro recibe.\n",
        "workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "\n",
        "WORK_FLAG = 1. El maestro envia el trabajo con tag WORK_FLAG.\n",
        "\n",
        "END_WORK_FLAG = 2. El maestro envia el trabajo \"0\" con tag END_WORK a todos los procesos esclavos para avisarles que ya no quedan trabajos que atender.\n",
        "\n",
        "Los esclavos lo reciben cualquiera sea el tag (MPI.ANY_TAG) y trabajan (bajo este contexto, descansan seg煤n ese valor, pudiendo ser 0, min_tiempo_sleep a max_tiempo_sleep). A continuaci贸n, si el tag resultaba ser END_WORK, finalizan.\n",
        "\n",
        "d) **驴C贸mo implementar铆a la funci贸n BLAS axpy() con este patr贸n?驴Ser铆a eficiente? Tips: Pide solo el planteo, no la implementaci贸n.**\n",
        "\n",
        "La operaci贸n axpy() es una operaci贸n lineal a partir de un escalar y 2 vectores.\n",
        "\n",
        "*Y <- aX + Y*\n",
        "\n",
        "MPI_Scatter(&sendbuf, sendcnt, sendtype, &recvbuf, ...)\n",
        "\n",
        "MPI_Gather(&sendbuf, sendcnt, sendtype, &recvbuf, ...)\n",
        "\n",
        "Podemos tener 2 procesos X e Y. A partir de MPI_Scatter distribuimos a cada proceso un vector. Paralelamente, se har铆a el c谩lculo axpy localmente posici贸n a posici贸n.\n",
        "A trav茅s de MPI_Gather, el proceso Y recibe los resultados \"parciales\" que ser铆a nuestro vector Y resultado.\n",
        "\n",
        "e) **驴Qu茅 sucede cuando solo ejecuta con una sola instancia?**\n",
        "\n",
        "Al definir NRO = 1, indicamos que la cantidad de procesos que se ejecutar谩n en paralelo sea uno. De esta forma, comm.Get_size() devuelve 1 efectivamente. El proceso maestro es parte, por lo que no hay \"empleados disponibles\". En el c贸digo, tras querer repartir el trabajo a 0 empleados, el numero de tareas es 0.\n",
        "\n",
        "numTasks = (numProcesses-1)*4\n",
        "\n",
        "numTasks = (1-1)*4 = 0\n",
        "\n",
        "f) Punto opcional: El c贸digo que ejecutan las instancias esclavas, tienen un error en su l贸gica. 驴C贸mo se podr铆a solucionar?"
      ],
      "metadata": {
        "id": "dUT7agILkKrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Armado del ambiente"
      ],
      "metadata": {
        "id": "yNdfhvbakUDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz9tN-lljqoj",
        "outputId": "c1ec9131-eeb2-4005-aacd-a7be6d556b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746527 sha256=890d6c838ec8a722078a70fad58703a5dcc61b4ecdb05bae3e538b8855aa7b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.5\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. C贸digo del programa en Lenguaje Python"
      ],
      "metadata": {
        "id": "c_l-CpO7jrxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Ejercicio2.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --------------------------------------------\n",
        "# Formulario\n",
        "Max_tiempo_sleep =   1#@param {type: \"slider\", min: 1, max: 10}\n",
        "Min_tiempo_sleep =   0#@param {type: \"slider\", min: 0, max: 10}\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------------------------------------------\n",
        "# Constantes de comunicacion\n",
        "WORK_FLAG = 1\n",
        "END_WORK_FLAG = 2\n",
        "# --------------------------------------------\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD # Instanciamos el tipo de comunicador a utilizar.\n",
        "    id = comm.Get_rank()  # Obtenemos el id como atributo del proceso que se ejecuta.\n",
        "\n",
        "    # Utilizamos el 0 para definir al procesos Maestro, cualquier otro id sera un esclavo.\n",
        "    if (id == 0) :\n",
        "        init() # Llamamos funcion init para eventos que requeriremos inicialmente solo 1 vez.\n",
        "        numProcesses = comm.Get_size()  # Obtenemos el numero de procesos totales ejecutados.\n",
        "        numTasks = (numProcesses-1)*4 # Se setea el numero de tareas.\n",
        "        workTimes = generateTasks(numTasks) # Se generan las tareas, en este caso seran\n",
        "        print(\"El jefe crea {} horas de descanso de sus empleados:\".format(workTimes.size), flush=True)\n",
        "        print(workTimes, flush=True)\n",
        "        initWork(comm, workTimes, numProcesses)\n",
        "    else:\n",
        "        doWork(comm)\n",
        "\n",
        "def generateTasks(numTasks):\n",
        "    #TODO: Cambiar la semilla del random para que se generen efectivamente diferentes numeros.\n",
        "    np.random.seed(1000)\n",
        "    return np.random.randint(low=Min_tiempo_sleep, high=Max_tiempo_sleep, size=numTasks)\n",
        "\n",
        "def init():\n",
        "  print()\n",
        "  print(\"Version MPI4py utilizada: {}\".format(MPI.Get_version()), flush=True)\n",
        "  print()\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print( \"Sistema de trabajo Suizo:\", flush=True)\n",
        "  print( \"------------------------------------\", flush=True)\n",
        "  print()\n",
        "\n",
        "def initWork(comm, workTimes, numProcesses):\n",
        "    totalWork = workTimes.size\n",
        "    workcount = 0\n",
        "    recvcount = 0\n",
        "\n",
        "    print(\"Jefe enviando las tareas iniciales:\", flush=True)\n",
        "    for id in range(1, numProcesses):\n",
        "        if workcount < totalWork:\n",
        "            work=workTimes[workcount]\n",
        "            comm.send(work, dest=id, tag=WORK_FLAG) # Envia mensaje de iniciar trabajo con el dato correspondiente del array.\n",
        "            workcount += 1\n",
        "            print(\"Jefe envia trabajo y {} hs de descanso al empleado {}.\".format(work, id), flush=True)\n",
        "    print( \"------------------------------------\", flush=True)\n",
        "\n",
        "    # Mientras haya trabajo, se recibe el resultado de los empleados y se sigue enviando MAS trabajo.\n",
        "    while (workcount < totalWork) :\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat) # Recivimos resultados de los empleados.\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source() # Obtenemos el identificador del empleado.\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "        #send next work\n",
        "        work=workTimes[workcount]\n",
        "        comm.send(work, dest=workerId, tag=WORK_FLAG)\n",
        "        workcount += 1\n",
        "        print(\"Jefe envia nuevo trabajo y {} hs de descanso al empleado {}.\".format(work, workerId), flush=True)\n",
        "\n",
        "    while (recvcount < totalWork):\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source()\n",
        "        print(\"Jefe recibe trabajo completado {} del empleado {}.\".format(workTime, workerId), flush=True)\n",
        "\n",
        "    for id in range(1, numProcesses):\n",
        "        comm.send(0, dest=id, tag=END_WORK_FLAG)\n",
        "\n",
        "\n",
        "def doWork(comm):\n",
        "    while(True):\n",
        "        stat = MPI.Status() # Obtiene el estado actual del empleado.\n",
        "        waitTime = comm.recv(source=0, tag=MPI.ANY_TAG, status=stat) # Obtiene lo enviado por el Jefe.\n",
        "        print(\"Soy el empleado con id {}, toca descanzo por {} hs.\".format(comm.Get_rank(), waitTime), flush=True)\n",
        "\n",
        "        if (stat.Get_tag() == END_WORK_FLAG):\n",
        "            print(\"Marca tarjeta el empleado {}.\".format(comm.Get_rank()), flush=True)\n",
        "            return\n",
        "        time.sleep(waitTime)\n",
        "        comm.send(waitTime, dest=0)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG-rAG5_kiA_",
        "outputId": "47df3b01-d38e-4607-94fc-4cfab9e7b810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Ejercicio2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Ejecuci贸n del programa"
      ],
      "metadata": {
        "id": "oumTsozbkqdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par谩metros de ejecuci贸n"
      ],
      "metadata": {
        "id": "ydfua9dxkvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "#@title Par谩metros de ejecuci贸n { vertical-output: true }\n",
        "NRO =   6#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python Ejercicio2.py"
      ],
      "metadata": {
        "id": "K17N9DXPkxMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Ejercicio Contar palabras"
      ],
      "metadata": {
        "id": "7H5zkFo7vPDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desarrollar un programa que permita obtener el valor m谩ximo de un conjunto dado de forma distribuida.\n",
        "\n",
        "Las condiciones a tener en cuenta son:\n",
        "\n",
        "Debe trabajar con al menos, 4 procesos.\n",
        "El resultado final debe ser informado en cada proceso.\n",
        "Implementar comunicaci贸n por Buffer"
      ],
      "metadata": {
        "id": "-FkLl9zxvRsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_tp4.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "count = 12\n",
        "sendbuf = None\n",
        "recvbuf_ = None # resultado reduce\n",
        "\n",
        "if rank == 0:\n",
        "  sendbuf = np.empty([size, count], dtype='i') # creo matriz\n",
        "  sendbuf[:,:] = np.random.randint(0, 100, size=(size, count)) # asigno valores\n",
        "\n",
        "  print(\"Matriz generada\\n\", sendbuf)\n",
        "  print(\"Maximo check = \", sendbuf.max()) # este deber铆a ser el resultado final.\n",
        "\n",
        "## ---------------------- SCATTER ----------------------------------------------\n",
        "\n",
        "recvbuf = np.empty(count, dtype='i') # array vacio, fila\n",
        "\n",
        "comm.Scatter(sendbuf, recvbuf, root=0) # scatter(matriz, vector, root)\n",
        "print(f\"Scatter | Proceso: {rank} - Data: {recvbuf}\")\n",
        "\n",
        "# todos los procesos recibieron una parte de la matriz \"sendbuf\",\n",
        "# y lo alojan en un array previamente vacio y secuencial \"recvbuf\"\n",
        "\n",
        "## ------------------------ REDUCE ---------------------------------------------\n",
        "\n",
        "if rank == 0:\n",
        "  recvbuf_ = np.empty([1, count], dtype='i') #matriz de fila 1\n",
        "\n",
        "comm.Reduce(recvbuf, recvbuf_, op=MPI.MAX, root=0) # reduce(vector, matriz, op, root)\n",
        "\n",
        "if rank == 0:\n",
        "  print(\"Reduce | Soy proceso root. Maximos de cada columna: \", recvbuf_) #resultado matriz\n",
        "\n",
        "# el proceso root recibe el array \"recvbuf\" de cada proceso y\n",
        "# junto con la operaci贸n para obtener los m谩ximos de cada COLUMNA de la matriz\n",
        "# se aloja el resultado en la matriz de una fila \"recvbuf_\".\n",
        "\n",
        "## ----------------------- BROADCAST -------------------------------------------\n",
        "\n",
        "if rank == 0:\n",
        "  max_final = recvbuf_.max() # variable con el maximo de maximos\n",
        "  data = np.array([max_final], dtype='i')\n",
        "else:\n",
        "  data = np.empty(1, dtype='i') # array de una sola posici贸n\n",
        "\n",
        "comm.Bcast(data, root=0)\n",
        "\n",
        "print(f\"Broadcast | Proceso: {rank} - Resultado final: {data[0]}\")\n",
        "\n",
        "# todos los procesos reciben el resultado final y lo informan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaB1xiB6vg0i",
        "outputId": "2c021353-2e72-4685-c227-e92a3552f0f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_tp4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NRO = 4\n",
        "! mpirun --oversubscribe --allow-run-as-root -np $NRO python mpi_tp4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxEzCTcwWHb",
        "outputId": "3e356396-e5c8-48f1-9908-46637f22169a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz generada\n",
            " [[69 88 62 54 66 43 31 52 17 20 22 69]\n",
            " [20 11  6 84 14 43 83 54 29 34 50 31]\n",
            " [94 23 19 29 27 17  0  3 89 75 13 83]\n",
            " [ 1 53 75 49 82 89 49 53 26 79 68 73]]\n",
            "Maximo check =  94\n",
            "Scatter | Proceso: 0 - Data: [69 88 62 54 66 43 31 52 17 20 22 69]\n",
            "Scatter | Proceso: 1 - Data: [20 11  6 84 14 43 83 54 29 34 50 31]\n",
            "Scatter | Proceso: 2 - Data: [94 23 19 29 27 17  0  3 89 75 13 83]\n",
            "Scatter | Proceso: 3 - Data: [ 1 53 75 49 82 89 49 53 26 79 68 73]\n",
            "Reduce | Soy proceso root. Maximos de cada columna:  [[94 88 75 84 82 89 83 54 89 79 68 83]]\n",
            "Broadcast | Proceso: 0 - Resultado final: 94\n",
            "Broadcast | Proceso: 1 - Resultado final: 94\n",
            "Broadcast | Proceso: 3 - Resultado final: 94\n",
            "Broadcast | Proceso: 2 - Resultado final: 94\n"
          ]
        }
      ]
    }
  ]
}